{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK4iXw4Q-C20",
        "outputId": "2eccf4e4-40ba-46d4-fb0b-c46afcd784d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ucg-6WpFm3hV_OGsE35Jq-UQrL3s4AJY\n",
            "To: /content/cnn_dm_extractive_compressed_5000.tar.gz\n",
            "100% 522M/522M [00:05<00:00, 93.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1ucg-6WpFm3hV_OGsE35Jq-UQrL3s4AJY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZY4HPD1B-RYq"
      },
      "outputs": [],
      "source": [
        "!tar -zxf cnn_dm_extractive_compressed_5000.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEeZAL90-tKv",
        "outputId": "6c4abe3a-d3de-4211-d932-cdb52248e47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cnn_dm_extractive_compressed_5000\n"
          ]
        }
      ],
      "source": [
        "%cd cnn_dm_extractive_compressed_5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gq6eqDMpawcD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "for i in range(3):\n",
        "  cmd = 'gzip -d test.'+str(i) + '.json.gz'\n",
        "  os.system(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w-AA0ivSdhin"
      },
      "outputs": [],
      "source": [
        "test_data = []\n",
        "import json\n",
        "for i in range(3):\n",
        "  data = 0\n",
        "  f = open('test.'+ str(i)+ '.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "  data = json.load(f)\n",
        "  test_data.extend(data)\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFR_h-Qqdu3r",
        "outputId": "6744963d-ad87-4e20-f2db-eace2c8173bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoUfXNceBZin",
        "outputId": "65c05d59-b714-4406-c6f4-365decab1da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n",
            "396438\n",
            "8985665\n"
          ]
        }
      ],
      "source": [
        "# Số từ\n",
        "no_word = 0\n",
        "# Số câu\n",
        "no_sent = 0\n",
        "# Số mẫu\n",
        "no_sample = 0\n",
        "for i in range(len(test_data)):\n",
        "  no_sample += 1\n",
        "  for j in range(len(test_data[i]['src'])):\n",
        "    no_sent += 1\n",
        "    for k in range(len(test_data[i]['src'][j])):\n",
        "      no_word += 1\n",
        "print(no_sample)\n",
        "print(no_sent)\n",
        "print(no_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xjNABCZ9ExKC"
      },
      "outputs": [],
      "source": [
        "for i in range(len(test_data)):\n",
        "  temp_list = []\n",
        "  for j in range(len(test_data[i]['src'])):\n",
        "    temp_list.append(' '.join(test_data[i]['src'][j]))\n",
        "  test_data[i]['src'] = temp_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DfddsYUYHnsF"
      },
      "outputs": [],
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4p4PCQEHsX_",
        "outputId": "cba5546c-e892-435a-bd56-5522333dd7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "laAnSuqiHux3"
      },
      "outputs": [],
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "63xYbnNJH-XJ"
      },
      "outputs": [],
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = text.replace(')', ') ')\n",
        "    text = text.replace('( ','(')\n",
        "    text = text.replace(\" '\",\"'\")\n",
        "    text = text.replace(\" n't\", \"n't\")\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iEQ2acyKIADu"
      },
      "outputs": [],
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fmog3UqoIBmx"
      },
      "outputs": [],
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    wnl = WordNetLemmatizer()\n",
        "    text= ' '.join([wnl.lemmatize(word) for word in text.split()])\n",
        "   # text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zSJKtoLID-z",
        "outputId": "8c3a6511-ab77-4073-890c-116977ea8c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"you'd\", 'how', 'you', \"didn't\", 'she', \"don't\", 'no', 'doing', 'was', 'doesn', 'do', 'not', 'those', 'than', 'hadn', 'most', 'is', 'under', 'after', 'didn', 'where', 'while', 'against', 'above', 'just', 'hers', 'have', 'too', 'being', 'were', 'itself', 'same', 'yourself', 't', \"doesn't\", \"won't\", 'our', 'all', \"you've\", 'and', 'weren', 'him', 'whom', 'up', 'these', 'his', \"should've\", \"wouldn't\", \"that'll\", 'there', 'theirs', 'below', 'of', 'about', 'themselves', 'the', 'between', 'won', 'which', 'out', 'it', 'had', 'here', 'yourselves', 'your', \"needn't\", 'did', 'any', 'my', 'they', 'd', 'again', 'been', 'from', \"mightn't\", 'because', 'i', 'mightn', 'own', 'each', 'until', 'y', 'with', 'that', 'herself', 'me', 'more', 'wouldn', 'through', 'shouldn', \"weren't\", 'this', 'during', 'he', 'who', 'wasn', 'couldn', 'why', 'ours', \"it's\", 'haven', 'or', \"hadn't\", \"you'll\", 'be', \"she's\", 'further', 'can', 'at', 'a', 'its', 'so', 'yours', 'in', 'then', 'nor', 'ma', 'am', 'has', 'an', 'to', 'on', 'having', 'isn', 'before', 'if', 'ourselves', 'them', 're', 'into', 'some', \"aren't\", \"wasn't\", \"you're\", 'for', 'over', 'shan', 'we', 'o', \"hasn't\", 'are', \"mustn't\", 'himself', 'by', 'once', \"couldn't\", 'few', 'ain', \"haven't\", 'needn', \"isn't\", 'both', 'when', 'her', 's', 'as', 'only', 'very', 'should', 've', 'aren', 'hasn', 'will', 'their', 'such', 'off', 'but', 'down', 'don', 'now', 'm', 'mustn', 'what', 'other', \"shan't\", 'does', 'll', 'myself', \"shouldn't\"}\n"
          ]
        }
      ],
      "source": [
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp99FJZgIX3U",
        "outputId": "305cf44c-bcf4-445f-a607-7ecf1c17788b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marseille , France ( CNN)The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane .\n",
            "marseille , france (cnn) the french prosecutor leading an investigation into the crash of germanwings flight 9525 insisted wednesday that he was not aware of any video footage from on board the plane .\n",
            "marseille  france cnn the french prosecutor leading an investigation into the crash of germanwings flight 9525 insisted wednesday that he was not aware of any video footage from on board the plane \n",
            "marseille france cnn french prosecutor leading investigation crash germanwings flight 9525 insisted wednesday aware video footage board plane\n",
            "marseille france cnn french prosecutor leading investigation crash germanwings flight 9525 insisted wednesday aware video footage board plane\n"
          ]
        }
      ],
      "source": [
        "text_t = test_data[0]['src'][0]\n",
        "print(text_t)\n",
        "text_t = denoise_text(text_t)\n",
        "print(text_t)\n",
        "text_t = remove_special_characters(text_t)\n",
        "print(text_t)\n",
        "text_t = remove_stopwords(text_t)\n",
        "print(text_t)\n",
        "text_t = simple_stemmer(text_t)\n",
        "print(text_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDak54t0_HF0",
        "outputId": "3adbfbd8-d52c-43e6-a237-633950f67560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Marseille , France ( CNN)The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane .', 'Marseille prosecutor Brice Robin told CNN that \" so far no videos were used in the crash investigation . \"', 'He added , \" A person who has such a video needs to immediately give it to the investigators . \"', \"Robin 's comments follow claims by two magazines , German daily Bild and French Paris Match , of a cell phone video showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French Alps .\", 'All 150 on board were killed .', 'Paris Match and Bild reported that the video was recovered from a phone at the wreckage site .', 'The two publications described the supposed video , but did not post it on their websites .', 'The publications said that they watched the video , which was found by a source close to the investigation .', '\" One can hear cries of \\' My God \\' in several languages , \" Paris Match reported .', '\" Metallic banging can also be heard more than three times , perhaps of the pilot trying to open the cockpit door with a heavy object .  ', 'Towards the end , after a heavy shake , stronger than the others , the screaming intensifies .', '\" It is a very disturbing scene , \" said Julian Reichelt , editor - in - chief of Bild online .', \"An official with France 's accident investigation agency , the BEA , said the agency is not aware of any such video .\", 'Jean - Marc Menichini , a French Gendarmerie spokesman in charge of communications on rescue efforts around the Germanwings crash site , told CNN that the reports were \" completely wrong \" and \" unwarranted . \"', 'Cell phones have been collected at the site , he said , but that they \" had n\\'t been exploited yet .', 'Menichini said he believed the cell phones would need to be sent to the Criminal Research Institute in Rosny sous - Bois , near Paris , in order to be analyzed by specialized technicians working hand - in - hand with investigators .', 'But none of the cell phones found so far have been sent to the institute , Menichini said .', 'Asked whether staff involved in the search could have leaked a memory card to the media , Menichini answered with a categorical \" no . \"', 'Reichelt told \" Erin Burnett :', 'Outfront \" that he had watched the video and stood by the report , saying Bild and Paris Match are \" very confident \" that the clip is real .', \"He noted that investigators only revealed they 'd recovered cell phones from the crash site after Bild and Paris Match published their reports .\", '\" That is something we did not know before . ...', 'Overall we can say many things of the investigation were n\\'t revealed by the investigation at the beginning , \" he said .', \"German airline Lufthansa confirmed Tuesday that co - pilot Andreas Lubitz had battled depression years before he took the controls of Germanwings Flight 9525 , which he 's accused of deliberately crashing last week in the French Alps .\", 'Lubitz told his Lufthansa flight training school in 2009 that he had a \" previous episode of severe depression , \" the airline said Tuesday .', 'Email correspondence between Lubitz and the school discovered in an internal investigation , Lufthansa said , included medical documents he submitted in connection with resuming his flight training .', \"The announcement indicates that Lufthansa , the parent company of Germanwings , knew of Lubitz 's battle with depression , allowed him to continue training and ultimately put him in the cockpit .\", 'Lufthansa , whose CEO Carsten Spohr previously said Lubitz was 100 % fit to fly , described its statement Tuesday as a \" swift and seamless clarification \" and said it was sharing the information and documents -- including training and medical records -- with public prosecutors .', 'Spohr traveled to the crash site Wednesday , where recovery teams have been working for the past week to recover human remains and plane debris scattered across a steep mountainside .', 'He saw the crisis center set up in Seyne - les - Alpes , laid a wreath in the village of Le Vernet , closer to the crash site , where grieving families have left flowers at a simple stone memorial .', 'Menichini told CNN late Tuesday that no visible human remains were left at the site but recovery teams would keep searching .', 'French President Francois Hollande , speaking Tuesday , said that it should be possible to identify all the victims using DNA analysis by the end of the week , sooner than authorities had previously suggested .', \"In the meantime , the recovery of the victims ' personal belongings will start Wednesday , Menichini said .\", 'Among those personal belongings could be more cell phones belonging to the 144 passengers and six crew on board .', 'Check out the latest from our correspondents .', \"The details about Lubitz 's correspondence with the flight school during his training were among several developments as investigators continued to delve into what caused the crash and Lubitz 's possible motive for downing the jet .\", 'A Lufthansa spokesperson told CNN on Tuesday that Lubitz had a valid medical certificate , had passed all his examinations and \" held all the licenses required . \"', \"Earlier , a spokesman for the prosecutor 's office in Dusseldorf , Christoph Kumpa , said medical records reveal\", \"Lubitz suffered from suicidal tendencies at some point before his aviation career and underwent psychotherapy before he got his pilot 's license .\", \"Kumpa emphasized there 's no evidence suggesting Lubitz was suicidal or acting aggressively before the crash .\", \"Investigators are looking into whether Lubitz feared his medical condition would cause him to lose his pilot 's license , a European government official briefed on the investigation told CNN on Tuesday .\", 'While flying was \" a big part of his life', ', \" the source said , it \\'s only one theory being considered .', 'Another source , a law enforcement official briefed on the investigation , also told CNN that authorities believe the primary motive for Lubitz to bring down the plane was that he feared he would not be allowed to fly because of his medical problems .', \"Lubitz 's girlfriend told investigators he had seen an eye doctor and a neuropsychologist , both of whom deemed him unfit to work recently and concluded he had psychological issues , the European government official said .\", \"But no matter what details emerge about his previous mental health struggles , there 's more to the story , said Brian Russell , a forensic psychologist .\", '\" Psychology can explain why somebody would turn rage inward on themselves about the fact that maybe they were n\\'t going to keep doing their job and they \\'re upset about that', 'and so they \\'re suicidal , \" he said .', '\" But there is no mental illness that explains why somebody then feels entitled to also take that rage and turn it outward on 149 other people who had nothing to do with the person \\'s problems . \"', 'Who was the captain of Germanwings Flight 9525 ?', \"CNN 's Margot Haddad reported from Marseille and Pamela Brown from Dusseldorf , while Laura Smith - Spark wrote from London .\", \"CNN 's Frederik Pleitgen , Pamela Boykoff , Antonia Mortensen , Sandrine Amiel and Anna - Maja Rappard contributed to this report .\"]\n"
          ]
        }
      ],
      "source": [
        "print(test_data[0]['src'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcaMzAGRMd5n",
        "outputId": "55b0bbc6-a969-4811-a7ab-006776cedac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-6073d530b3c9>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "preprocesed_test_data = []\n",
        "for i in range(len(test_data)):\n",
        "  temp_list = []\n",
        "  for j in range(len(test_data[i]['src'])):\n",
        "    text_s = test_data[i]['src'][j]\n",
        "    text_s = denoise_text(text_s)\n",
        "    text_s = remove_special_characters(text_s)\n",
        "    text_s = remove_stopwords(text_s)\n",
        "    text_s = simple_stemmer(text_s)\n",
        "    temp_list.append(text_s)\n",
        "  preprocesed_test_data.append(temp_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xd2YTacWTZa",
        "outputId": "fe564342-c1a2-4d35-f2cb-df9c26b79345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Two police officers have sustained injuries after attempting to close down an enormous 1000 person rave in Sydney 's East .\", 'At about 10.30pm on Saturday night , police received a number of complaints about a dangerously large party at an abandoned industrial area on McPherson Street in Botany .', 'Police were forced to use capsicum spray on a number of the attendees and one officer had to have a piece of glass removed from his head after having a bottle thrown at him .', 'Police officers have shut down an enormous 1000 rave on McPherson Street in Botany , Sydney .', \"Botany Officers received calls to an abandoned industrial area in Sydney 's East where a rave was occurring .\", 'On police officer was hit by a bottle and was taken to hospital to have glass removed from his head .', 'They were forced to use capsicum spray on partygoers after the crowd became violent .', 'The male officer was treated at the scene and later had a piece of glass removed from is head at the Prince of Wales Hospital .', 'According to NSW police media police were assisted by back up officers as well as the Riot Squad and Dog Squads .', 'After further safety concerns , Public Order , Operations Support Group and Traffic and Highway patrol were also in attendance .', 'After further safety concerns , Public Order , Operations Support Group and Highway patrol also attended .', 'Most of the partygoers were moved from the scene relatively easily , but a number began to throw glass bottles .', 'A number of the partygoers were treated by ambulance paramedics for minor capsicum spray contamination .', 'Most of the partygoers were moved from the scene relatively easily , but a number began to throw glass bottles , forcing police to resort to capsicum spray .', 'A 26-year - old woman was arrested after she allegedly assaulted an officer .', 'She is being interviewed by police at Botany Bay Police Station .', 'A number of the partygoers were treated by ambulance paramedics for minor capsicum spray contamination .', 'Police are currently investigating whether the party was advertised on social media .']\n"
          ]
        }
      ],
      "source": [
        "print(((test_data[1902]['src'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD5FwjcEcN_A",
        "outputId": "bc2a105d-489c-4082-c835-f40b6aa6bf5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['two police officer sustained injury attempting close enormous 1000 person rave sydney east', '1030pm saturday night police received number complaint dangerously large party abandoned industrial area mcpherson street botany', 'police forced use capsicum spray number attendee one officer piece glass removed head bottle thrown', 'police officer shut enormous 1000 rave mcpherson street botany sydney', 'botany officer received call abandoned industrial area sydney east rave occurring', 'police officer hit bottle taken hospital glass removed head', 'forced use capsicum spray partygoer crowd became violent', 'male officer treated scene later piece glass removed head prince wale hospital', 'according nsw police medium police assisted back officer well riot squad dog squad', 'safety concern public order operation support group traffic highway patrol also attendance', 'safety concern public order operation support group highway patrol also attended', 'partygoer moved scene relatively easily number began throw glass bottle', 'number partygoer treated ambulance paramedic minor capsicum spray contamination', 'partygoer moved scene relatively easily number began throw glass bottle forcing police resort capsicum spray', '26year old woman arrested allegedly assaulted officer', 'interviewed police botany bay police station', 'number partygoer treated ambulance paramedic minor capsicum spray contamination', 'police currently investigating whether party advertised social medium']\n"
          ]
        }
      ],
      "source": [
        "print((preprocesed_test_data[1902]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F64d_xOeegTm",
        "outputId": "2d8d196f-4392-4bff-bc24-cc0da62b4e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "z6iRUvm7eWaR"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "ROUGE = Rouge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVqfJbVjvqDQ",
        "outputId": "7de5a8b5-4a13-4cba-d714-cb85d0f46610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "target = []\n",
        "for i in range(len(test_data)):\n",
        "  target.append(test_data[i]['tgt'])\n",
        "print(len(target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0it-826I-dZ-",
        "outputId": "aa8134f2-1d57-4b79-8d20-fef5b9f047be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "3\n",
            "2.64365915223257\n"
          ]
        }
      ],
      "source": [
        "no_sent_avg = []\n",
        "for i in range(len(test_data)):\n",
        "  no_sent_avg.append(test_data[i]['labels'].count(1))\n",
        "print(min(no_sent_avg))\n",
        "print(max(no_sent_avg))\n",
        "print(sum(no_sent_avg)/len(no_sent_avg))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRTZkpm7t5rk"
      },
      "source": [
        "##Lead-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPzqyPUpt44N",
        "outputId": "523fb93e-c9aa-49f8-ec7f-be53f42f74e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "lead_3 = []\n",
        "for i in range(len(test_data)):\n",
        "  lead_3.append(' '.join(test_data[i]['src'][:3]))\n",
        "print(len(lead_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWbBp5M60_mr",
        "outputId": "212cf645-ad6f-4841-f4dc-6a3d90ec49f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'r': 0.4649200005186179, 'p': 0.3407655513464968, 'f': 0.38258801182719576}, 'rouge-2': {'r': 0.2064405152358979, 'p': 0.13970924174105676, 'f': 0.16057365278271576}, 'rouge-l': {'r': 0.42308306338118934, 'p': 0.31004252061991255, 'f': 0.3481148201378483}}\n"
          ]
        }
      ],
      "source": [
        "print(ROUGE.get_scores(lead_3,target,avg = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhQALrqH9Dj3"
      },
      "source": [
        "##BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZEnhym11c3H"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import Counter\n",
        "def generate_summary_BoW(sentences, n):\n",
        "# Tokenize the text into individual sentences\n",
        "\n",
        "  # Tokenize each sentence into individual words and remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # the following line would tokenize each sentence from sentences into individual words using the word_tokenize function of nltk.tokenize module\n",
        "  # Then removes any stop words and non-alphanumeric characters from the resulting list of words and converts them all to lowercase\n",
        "  for sentence in sentences:\n",
        "    words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n",
        "\n",
        "  # Compute the frequency of each word\n",
        "  word_freq = Counter(words)\n",
        "\n",
        "  # Compute the score for each sentence based on the frequency of its words\n",
        "  # After this block of code is executed, sentence_scores will contain the scores of each sentence in the given text,\n",
        "  # where each score is a sum of the frequency counts of its constituent words\n",
        "\n",
        "  # empty dictionary to store the scores for each sentence\n",
        "  sentence_scores = {}\n",
        "  i = 0\n",
        "  for sentence in sentences:\n",
        "    sentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n",
        "    sentence_score = sum([word_freq[word] for word in sentence_words])\n",
        "    if len(sentence_words) < 30:\n",
        "      sentence_scores[i] = sentence_score\n",
        "    i+=1\n",
        "\n",
        "  # checks if the length of the sentence_words list is less than 30 (parameter can be adjusted based on the desired length of summary sentences)\n",
        "  # If condition -> true, score of the current sentence is added to the sentence_scores dictionary with the sentence itself as the key\n",
        "  # This is to filter out very short sentences that may not provide meaningful information for summary generation\n",
        "\n",
        "  # Select the top n sentences with the highest scores\n",
        "  summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n",
        "  summary = summary_sentences\n",
        "\n",
        "  return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv7Ey1IvCxKu",
        "outputId": "580b618f-c09e-43d3-a7d1-ed9ad8eeb999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "BoW = []\n",
        "for i in range(len(test_data)):\n",
        "  BoW_list = generate_summary_BoW(preprocesed_test_data[i], 3)\n",
        "  BoW_list.sort()\n",
        "  BoW.append(' '.join(test_data[i]['src'][j] for j in BoW_list))\n",
        "print(len(BoW))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_alI07SEckh",
        "outputId": "e9ee7b7e-03b9-4013-df19-4204150e69df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.3155913752204953, 'p': 0.2695332847046216, 'f': 0.28052874082663204}, 'rouge-2': {'r': 0.10382827456152627, 'p': 0.08185034696270334, 'f': 0.08739717305327407}, 'rouge-l': {'r': 0.28404109777588243, 'p': 0.2429472225620566, 'f': 0.2526621690635056}}\n"
          ]
        }
      ],
      "source": [
        "print(ROUGE.get_scores(BoW,target,avg = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLMguFVGFvMJ"
      },
      "source": [
        "##TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ4H_byJFxOp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def generate_summary_TFIDF(sentences, n):\n",
        "# Tokenize the text into individual sentences\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words='english')\n",
        "  tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "\n",
        "  # empty dictionary to store the scores for each sentence\n",
        "  sentence_scores = {}\n",
        "  i = 0\n",
        "  for sentence in sentences:\n",
        "    sentence_score = np.sum(tfidf_matrix[i])\n",
        "    sentence_scores[i] = sentence_score\n",
        "    i+=1\n",
        "\n",
        "  # checks if the length of the sentence_words list is less than 30 (parameter can be adjusted based on the desired length of summary sentences)\n",
        "  # If condition -> true, score of the current sentence is added to the sentence_scores dictionary with the sentence itself as the key\n",
        "  # This is to filter out very short sentences that may not provide meaningful information for summary generation\n",
        "\n",
        "  # Select the top n sentences with the highest scores\n",
        "  summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n",
        "  summary = summary_sentences\n",
        "\n",
        "  return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xE8J7chHqbC",
        "outputId": "36352bcd-247b-4745-c2cc-759692b378e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 13, 2]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_summary_TFIDF(test_data[1902]['src'], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HB2sDwqJuUf",
        "outputId": "4e1d1d7b-d417-4154-849d-395a8e0c14c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "tfidf = []\n",
        "for i in range(len(test_data)):\n",
        "  tfidf_list = generate_summary_TFIDF(preprocesed_test_data[i], 3)\n",
        "  tfidf_list.sort()\n",
        "  tfidf.append(' '.join(test_data[i]['src'][j] for j in tfidf_list))\n",
        "print(len(tfidf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fM2OCX1J2-L",
        "outputId": "5b4b1d2f-90a4-4a81-a6ec-0109344ae66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.4237596804736506, 'p': 0.24273189435320042, 'f': 0.3005580707842897}, 'rouge-2': {'r': 0.1521705055010271, 'p': 0.08044434671902961, 'f': 0.10144538745548726}, 'rouge-l': {'r': 0.37748399077931954, 'p': 0.21650603153697517, 'f': 0.2679125773332579}}\n"
          ]
        }
      ],
      "source": [
        "print(ROUGE.get_scores(tfidf,target,avg = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2w6YpKad2qb"
      },
      "source": [
        "## LSA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq_0d-uWd7_q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse.linalg import svds\n",
        "from numpy.linalg import svd\n",
        "\n",
        "\n",
        "def low_rank_svd(matrix, singular_count=2):\n",
        "    u, s, vt = svds(matrix, k = singular_count)\n",
        "    return u, s, vt\n",
        "\n",
        "def generate_summary_LSA(sentences, n,vector):\n",
        "# Tokenize the text into individual sentences\n",
        "\n",
        "  sent_vector=[]\n",
        "  for i in sentences:\n",
        "      plus=np.zeros((300,))\n",
        "      for j in i.split():\n",
        "          if j in vector:\n",
        "            plus+= vector[j]\n",
        "      plus = plus/(len(i.split())+1)\n",
        "      sent_vector.append(plus)\n",
        "  sent_vector = np.array(sent_vector)\n",
        "  # Apply Singular Value Decomposition (SVD)\n",
        "  u, s, vt = low_rank_svd(sent_vector.T, singular_count=2)\n",
        "  term_topic_mat, singular_values, topic_document_mat = u, s, vt\n",
        "\n",
        "  salience_scores = np.sqrt(np.dot(np.square(singular_values),\n",
        "                                 np.square(topic_document_mat)))\n",
        "\n",
        "\n",
        "  top_sentence_indices = (-salience_scores).argsort()[:n]\n",
        "\n",
        "  return top_sentence_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm-Eoj27MRGW"
      },
      "source": [
        "## Clusterring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HaqIxIOMnyB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def generate_summary_Cluster(sentences, n, vector):\n",
        "# Tokenize the text into individual sentences\n",
        "\n",
        "  sent_vector=[]\n",
        "  for i in sentences:\n",
        "      plus=np.zeros((300,))\n",
        "      for j in i.split():\n",
        "          if j in vector:\n",
        "            plus+= vector[j]\n",
        "      plus = plus/(len(i.split())+1)\n",
        "      sent_vector.append(plus)\n",
        "  sent_vector = np.array(sent_vector)\n",
        "\n",
        "  km_model = KMeans(n_clusters=n, n_init = 10)\n",
        "  km_model.fit(sent_vector)\n",
        "\n",
        "  cluster_labels = km_model.labels_\n",
        "  cluster_sentences = {i: [] for i in range(n)}\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "      cluster_sentences[cluster_labels[i]].append((i, sentence))\n",
        "  summary_sentences = []\n",
        "  for cluster in cluster_sentences.values():\n",
        "    representative_sentence = max(cluster, key=lambda x: len(x[1]))[0]\n",
        "    summary_sentences.append(representative_sentence)\n",
        "\n",
        "\n",
        "  return summary_sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nUKdku3PTjL"
      },
      "outputs": [],
      "source": [
        "# generate_summary_Cluster(test_data[1902]['src'], 3,w2v_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRj8naXSWJrU"
      },
      "source": [
        "##TextRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15XdeFd7WJra"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def generate_summary_TextRank(sentences, n, vector):\n",
        "# Tokenize the text into individual sentences\n",
        "  sent_vector=[]\n",
        "  for i in sentences:\n",
        "      plus=np.zeros((300,))\n",
        "      for j in i.split():\n",
        "          if j in vector:\n",
        "            plus+= vector[j]\n",
        "      plus = plus/(len(i.split())+1)\n",
        "      sent_vector.append(plus)\n",
        "  sent_vector = np.array(sent_vector)\n",
        "\n",
        "  # similarity matrix\n",
        "  sim_matrix = np.zeros([len(sentences), len(sentences)])\n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "     if i != j:\n",
        "        sim_matrix[i][j] = cosine_similarity(sent_vector[i].reshape(1,300), sent_vector[j].reshape(1,300))\n",
        "\n",
        "  similarity_graph = nx.from_numpy_array(sim_matrix)\n",
        "\n",
        "  scores = nx.pagerank(similarity_graph)\n",
        "  ranked_sentences = sorted(((score, index) for index, score in scores.items()), reverse=True)\n",
        "\n",
        "  summary = []\n",
        "  for score, index in ranked_sentences[:n]:\n",
        "    summary.append(index)\n",
        "  return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "ZcC_qLOwWJra",
        "outputId": "6bb4762c-c4bd-4a25-f267-8b762dc39229"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-69bdfc7d723e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgenerate_summary_TextRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1902\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'w2v_vector' is not defined"
          ]
        }
      ],
      "source": [
        "# import time\n",
        "# start = time.time()\n",
        "# generate_summary_TextRank(test_data[1902]['src'], 3, w2v_vector)\n",
        "# print(time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfqnfvwaMHD8"
      },
      "source": [
        "##Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4D8byNOMB2u"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfyMcEQqRq8O",
        "outputId": "254c5464-497b-44b3-95b2-60138690ddcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fasttext-wiki-news-subwords-300': {'num_records': 999999, 'file_size': 1005007116, 'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py', 'license': 'https://creativecommons.org/licenses/by-sa/3.0/', 'parameters': {'dimension': 300}, 'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).', 'read_more': ['https://fasttext.cc/docs/en/english-vectors.html', 'https://arxiv.org/abs/1712.09405', 'https://arxiv.org/abs/1607.01759'], 'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af', 'file_name': 'fasttext-wiki-news-subwords-300.gz', 'parts': 1}, 'conceptnet-numberbatch-17-06-300': {'num_records': 1917247, 'file_size': 1225497562, 'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py', 'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt', 'parameters': {'dimension': 300}, 'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.', 'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972', 'https://github.com/commonsense/conceptnet-numberbatch', 'http://conceptnet.io/'], 'checksum': 'fd642d457adcd0ea94da0cd21b150847', 'file_name': 'conceptnet-numberbatch-17-06-300.gz', 'parts': 1}, 'word2vec-ruscorpora-300': {'num_records': 184973, 'file_size': 208427381, 'base_dataset': 'Russian National Corpus (about 250M words)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py', 'license': 'https://creativecommons.org/licenses/by/4.0/deed.en', 'parameters': {'dimension': 300, 'window_size': 10}, 'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.', 'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS', 'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models', 'http://rusvectores.org/en/', 'https://github.com/RaRe-Technologies/gensim-data/issues/3'], 'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4', 'file_name': 'word2vec-ruscorpora-300.gz', 'parts': 1}, 'word2vec-google-news-300': {'num_records': 3000000, 'file_size': 1743563840, 'base_dataset': 'Google News (about 100 billion words)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py', 'license': 'not found', 'parameters': {'dimension': 300}, 'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\", 'read_more': ['https://code.google.com/archive/p/word2vec/', 'https://arxiv.org/abs/1301.3781', 'https://arxiv.org/abs/1310.4546', 'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'], 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef', 'file_name': 'word2vec-google-news-300.gz', 'parts': 1}, 'glove-wiki-gigaword-50': {'num_records': 400000, 'file_size': 69182535, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 50}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813', 'file_name': 'glove-wiki-gigaword-50.gz', 'parts': 1}, 'glove-wiki-gigaword-100': {'num_records': 400000, 'file_size': 134300434, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 100}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '40ec481866001177b8cd4cb0df92924f', 'file_name': 'glove-wiki-gigaword-100.gz', 'parts': 1}, 'glove-wiki-gigaword-200': {'num_records': 400000, 'file_size': 264336934, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 200}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '59652db361b7a87ee73834a6c391dfc1', 'file_name': 'glove-wiki-gigaword-200.gz', 'parts': 1}, 'glove-wiki-gigaword-300': {'num_records': 400000, 'file_size': 394362229, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 300}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '29e9329ac2241937d55b852e8284e89b', 'file_name': 'glove-wiki-gigaword-300.gz', 'parts': 1}, 'glove-twitter-25': {'num_records': 1193514, 'file_size': 109885004, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 25}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '50db0211d7e7a2dcd362c6b774762793', 'file_name': 'glove-twitter-25.gz', 'parts': 1}, 'glove-twitter-50': {'num_records': 1193514, 'file_size': 209216938, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 50}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'c168f18641f8c8a00fe30984c4799b2b', 'file_name': 'glove-twitter-50.gz', 'parts': 1}, 'glove-twitter-100': {'num_records': 1193514, 'file_size': 405932991, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 100}, 'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'b04f7bed38756d64cf55b58ce7e97b15', 'file_name': 'glove-twitter-100.gz', 'parts': 1}, 'glove-twitter-200': {'num_records': 1193514, 'file_size': 795373100, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 200}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'e52e8392d1860b95d5308a525817d8f9', 'file_name': 'glove-twitter-200.gz', 'parts': 1}, '__testing_word2vec-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.', 'parameters': {'dimensions': 50}, 'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.', 'read_more': [], 'checksum': '534dcb8b56a360977a269b7bfc62d124', 'file_name': '__testing_word2vec-matrix-synopsis.gz', 'parts': 1}}\n"
          ]
        }
      ],
      "source": [
        "print((gensim.downloader.info()['models']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXbOZ7XEV3j5",
        "outputId": "644ae8ab-59ac-4eed-e3af-d95946c65861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "w2v_vector = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEFL8CG7ZwJT",
        "outputId": "a492d4da-20ee-45e6-f6e3-cdec50ae6f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.09277344  0.08007812  0.13671875 -0.33203125 -0.06054688 -0.22949219\n",
            " -0.04589844 -0.25390625 -0.25195312  0.22265625  0.23046875 -0.23046875\n",
            " -0.18457031  0.09277344 -0.171875   -0.02758789  0.01525879 -0.14355469\n",
            " -0.00173187 -0.13378906  0.08105469 -0.07470703  0.15332031  0.00457764\n",
            "  0.12890625 -0.24511719 -0.02929688 -0.07763672  0.23339844 -0.26171875\n",
            "  0.26757812 -0.11962891  0.04150391  0.00352478 -0.2421875  -0.24121094\n",
            "  0.13574219  0.19824219 -0.10107422  0.26367188  0.01708984 -0.109375\n",
            "  0.04614258  0.09765625 -0.10791016  0.08544922 -0.17773438 -0.11132812\n",
            " -0.04663086  0.20214844  0.13574219  0.06689453  0.08300781  0.1015625\n",
            " -0.1796875  -0.18066406 -0.26757812  0.0703125  -0.1953125  -0.16699219\n",
            "  0.0859375   0.31054688 -0.04077148 -0.10644531 -0.00128937  0.17285156\n",
            " -0.07910156  0.04394531 -0.05688477 -0.16210938  0.16796875  0.27734375\n",
            "  0.0133667   0.02270508  0.02148438  0.07568359  0.11376953 -0.15625\n",
            "  0.17578125 -0.3984375   0.26171875  0.0456543   0.11328125  0.08398438\n",
            " -0.29492188 -0.02099609 -0.06054688 -0.00473022  0.13671875 -0.00315857\n",
            "  0.06005859 -0.15820312  0.08056641  0.00811768 -0.10644531  0.0324707\n",
            "  0.06933594  0.06298828  0.04052734 -0.12792969 -0.01269531 -0.07910156\n",
            "  0.15039062  0.07128906  0.3125     -0.11523438  0.00769043 -0.00765991\n",
            "  0.07373047 -0.32226562 -0.15917969  0.09130859 -0.07617188 -0.18066406\n",
            " -0.16308594 -0.015625   -0.11376953 -0.24609375  0.0859375   0.33789062\n",
            "  0.14550781  0.22460938 -0.24902344 -0.05566406  0.06982422  0.00485229\n",
            " -0.09570312 -0.20898438  0.21875     0.12207031  0.08642578 -0.34375\n",
            " -0.12988281  0.05737305 -0.11621094 -0.03112793 -0.01397705 -0.05737305\n",
            " -0.18359375  0.3359375   0.07470703 -0.07080078 -0.11328125  0.11865234\n",
            " -0.13867188  0.05078125 -0.09472656 -0.14453125 -0.18164062 -0.19824219\n",
            "  0.3515625   0.18847656 -0.10839844 -0.16113281 -0.03051758  0.17285156\n",
            " -0.02502441 -0.10058594  0.07226562 -0.12890625 -0.09570312 -0.11230469\n",
            " -0.05053711  0.01184082  0.04052734  0.09277344  0.09423828 -0.28125\n",
            "  0.3359375  -0.1796875   0.22949219 -0.04956055  0.03833008 -0.07714844\n",
            " -0.015625   -0.08349609 -0.10302734  0.20019531 -0.41992188 -0.09912109\n",
            "  0.31054688  0.10498047 -0.12011719 -0.2578125   0.11523438 -0.15332031\n",
            " -0.01721191  0.296875    0.21289062 -0.00976562  0.04785156 -0.02758789\n",
            " -0.38476562 -0.07763672 -0.03295898  0.03027344 -0.09375    -0.15917969\n",
            "  0.07910156  0.13867188 -0.01928711  0.125       0.0703125   0.01953125\n",
            "  0.03955078 -0.17578125  0.10546875  0.0133667   0.09716797  0.01531982\n",
            "  0.04003906 -0.14160156  0.10107422  0.15625    -0.24121094 -0.05151367\n",
            "  0.21679688  0.17285156 -0.34570312  0.03344727 -0.20117188 -0.03125\n",
            "  0.07714844 -0.11669922  0.04345703 -0.15332031  0.04321289  0.21777344\n",
            "  0.11621094  0.06933594  0.24121094  0.04223633 -0.30078125  0.26171875\n",
            "  0.03564453 -0.24121094  0.27539062  0.06030273  0.09375     0.01287842\n",
            "  0.10253906 -0.02416992 -0.04443359 -0.17285156 -0.18261719  0.07421875\n",
            " -0.08984375 -0.17773438  0.14257812  0.04418945  0.08251953  0.01574707\n",
            "  0.15234375  0.04907227  0.18359375 -0.06640625  0.01953125 -0.17089844\n",
            "  0.03759766 -0.12011719 -0.03491211  0.01373291 -0.25390625  0.02929688\n",
            "  0.32226562  0.09082031 -0.1796875  -0.07666016 -0.20800781 -0.1640625\n",
            " -0.05566406  0.12353516 -0.2265625   0.29296875  0.09423828  0.01586914\n",
            "  0.22460938 -0.05004883  0.07128906 -0.10546875 -0.08203125  0.00239563\n",
            "  0.14746094  0.06054688  0.13476562  0.05371094  0.01086426 -0.14257812\n",
            "  0.26953125  0.03112793 -0.08105469 -0.12695312 -0.03344727  0.10693359\n",
            "  0.05395508  0.11816406 -0.11962891 -0.23632812  0.01635742 -0.14941406]\n"
          ]
        }
      ],
      "source": [
        "print(w2v_vector['police'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igQtAC-xZz43",
        "outputId": "c663c63e-7368-48ce-c964-fb4abe2e69a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "LSA = []\n",
        "for i in range(len(test_data)):\n",
        "  LSA_list = generate_summary_LSA(preprocesed_test_data[i], 3, w2v_vector)\n",
        "  LSA_list.sort()\n",
        "  LSA.append(' '.join(test_data[i]['src'][j] for j in LSA_list))\n",
        "print(len(LSA))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5oTRldOq_4X",
        "outputId": "2466028d-9fab-47a6-a2b5-746b5e6f8cd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.2343509943963394, 'p': 0.26956759887676435, 'f': 0.23978275123561}, 'rouge-2': {'r': 0.06587366996646216, 'p': 0.07160694562593609, 'f': 0.06477754442114535}, 'rouge-l': {'r': 0.2121821908578747, 'p': 0.24463034502170222, 'f': 0.21724481623752304}}\n"
          ]
        }
      ],
      "source": [
        "print(ROUGE.get_scores(LSA,target,avg = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf9C8G5U3H0z",
        "outputId": "8b3e5ff5-090a-41e4-d0ea-f383165ee17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "Cluster = []\n",
        "for i in range(len(test_data)):\n",
        "  Cluster_list = generate_summary_Cluster(preprocesed_test_data[i], 3,w2v_vector)\n",
        "  Cluster_list.sort()\n",
        "  Cluster.append(' '.join(test_data[i]['src'][j] for j in Cluster_list))\n",
        "print(len(Cluster))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1cx4GtK3MWp",
        "outputId": "badf2a0a-6fe2-410e-c28b-ce6df45e1d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.3668246291822515, 'p': 0.23951113822895578, 'f': 0.2822982274666079}, 'rouge-2': {'r': 0.11797239779251503, 'p': 0.07292654036609279, 'f': 0.0868843182382333}, 'rouge-l': {'r': 0.3246608649113472, 'p': 0.21219134936212966, 'f': 0.2499754332948612}}\n"
          ]
        }
      ],
      "source": [
        "print(ROUGE.get_scores(Cluster,target,avg = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I47Rr2Ct7U2Z",
        "outputId": "e40a23d9-daad-47cb-b397-e3301a71cec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "TextRank = []\n",
        "for i in range(len(test_data)):\n",
        "  TextRank_list = generate_summary_TextRank(test_data[i]['src'], 3,w2v_vector)\n",
        "  TextRank_list.sort()\n",
        "  TextRank.append(' '.join(test_data[i]['src'][j] for j in TextRank_list))\n",
        "print(len(TextRank))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LWrD1By0jL8J",
        "outputId": "a9601b53-8bfd-4285-c21a-58b2ebcdd293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'r': 0.38783570630949893, 'p': 0.2585435229690131, 'f': 0.3011514621038352}, 'rouge-2': {'r': 0.12934290950533692, 'p': 0.07824761724005334, 'f': 0.09367646866411687}, 'rouge-l': {'r': 0.3449039952641219, 'p': 0.2299694918550534, 'f': 0.26783065649852184}}\n"
          ]
        }
      ],
      "source": [
        "print(ROUGE.get_scores(TextRank,target,avg = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf-Idf"
      ],
      "metadata": {
        "id": "tp3a3wctFKqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "\n",
        "def low_rank_svd(matrix, singular_count=2):\n",
        "    u, s, vt = svds(matrix, k=singular_count)\n",
        "    return u, s, vt\n",
        "\n",
        "def generate_summary_LSA(sentences, n):\n",
        "    # Tokenize the text into individual sentences\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Apply Singular Value Decomposition (SVD)\n",
        "    u, s, vt = low_rank_svd(tfidf_matrix.T, singular_count=2)\n",
        "    term_topic_mat, singular_values, topic_document_mat = u, s, vt\n",
        "    salience_scores = np.sqrt(np.dot(np.square(singular_values),\n",
        "                                     np.square(topic_document_mat)))\n",
        "\n",
        "    top_sentence_indices = salience_scores.argsort()[-3:][::-1]\n",
        "    summary_sentences = [index for index in top_sentence_indices]\n",
        "\n",
        "    return summary_sentences"
      ],
      "metadata": {
        "id": "Fst-q95-FUGn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary_LSA(test_data[1902]['src'], 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD-o_rtD0dBZ",
        "outputId": "aaae7fa7-85ee-452e-917d-cede45cd6ba3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16, 12, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSA = []\n",
        "for i in range(len(test_data)):\n",
        "  LSA_list = generate_summary_LSA(test_data[i]['src'], 3)\n",
        "  LSA.append(' '.join(test_data[i]['src'][j] for j in LSA_list))\n",
        "print(len(LSA))"
      ],
      "metadata": {
        "id": "_LcUDIRIXnfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "771cc30d-76d1-4469-8fc5-c975488dc3b6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ROUGE.get_scores(LSA,target,avg = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMbKRj8Y09eb",
        "outputId": "adccb293-7a3f-412e-dbdf-8d90dbb8026a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.3320378066044688, 'p': 0.336385137596425, 'f': 0.32045750215770186}, 'rouge-2': {'r': 0.1298179208804682, 'p': 0.12148597658180596, 'f': 0.11917612028069614}, 'rouge-l': {'r': 0.3012145214030458, 'p': 0.3058115738026821, 'f': 0.2909370505359382}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def generate_summary_Cluster(sentences, n):\n",
        "# Tokenize the text into individual sentences\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words='english')\n",
        "  tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "  km_model = KMeans(n_clusters=n)\n",
        "  km_model.fit(tfidf_matrix)\n",
        "\n",
        "  cluster_labels = km_model.labels_\n",
        "\n",
        "  cluster_sentences = {i: [] for i in range(n)}\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "      cluster_sentences[cluster_labels[i]].append((i, sentence))\n",
        "\n",
        "  summary_sentences = []\n",
        "  for cluster in cluster_sentences.values():\n",
        "    representative_sentence = max(cluster, key=lambda x: len(x[1]))[0]\n",
        "    summary_sentences.append(representative_sentence)\n",
        "\n",
        "\n",
        "  return summary_sentences"
      ],
      "metadata": {
        "id": "A1JSWMW7X-LN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary_Cluster(test_data[1902]['src'], 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFFf9-D105E-",
        "outputId": "42b5fb8f-4746-452b-ecc3-769db81a8e17"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Cluster = []\n",
        "for i in range(len(test_data)):\n",
        "  Cluster_list = generate_summary_Cluster(test_data[i]['src'], 3)\n",
        "  Cluster.append(' '.join(test_data[i]['src'][j] for j in Cluster_list))\n",
        "print(len(Cluster))"
      ],
      "metadata": {
        "id": "dq7ElNdSYA-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4de160-f2f0-46d8-afc3-7f199f03d45e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ROUGE.get_scores(Cluster,target,avg = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVEzLuIH1o50",
        "outputId": "1150de7f-d758-48c1-eeae-afe59ca4c6c3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.40730160546403876, 'p': 0.2335799967740494, 'f': 0.28997199933265166}, 'rouge-2': {'r': 0.13627565883403228, 'p': 0.07180281126232194, 'f': 0.09104532126005666}, 'rouge-l': {'r': 0.3606081794940306, 'p': 0.20686223259364697, 'f': 0.2567549792777818}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import networkx as nx\n",
        "\n",
        "def generate_summary_TextRank(sentences, n):\n",
        "# Tokenize the text into individual sentences\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words='english')\n",
        "  tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "  sentences_matrix = tfidf_matrix*tfidf_matrix.T\n",
        "\n",
        "  similarity_graph = nx.from_numpy_array(sentences_matrix)\n",
        "\n",
        "  scores = nx.pagerank(similarity_graph)\n",
        "  ranked_sentences = sorted(((score, index) for index, score in scores.items()), reverse=True)\n",
        "\n",
        "  summary = []\n",
        "  for score, index in ranked_sentences[:n]:\n",
        "    summary.append(index)\n",
        "  return summary\n"
      ],
      "metadata": {
        "id": "bU4PDZ5nYEhO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary_TextRank(test_data[1902]['src'], 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt4DaAb83w6N",
        "outputId": "5f8e9fb7-0a71-43e5-e0af-a9c10cfdf104"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 13, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextRank = []\n",
        "for i in range(len(test_data)):\n",
        "  TextRank_list = generate_summary_TextRank(test_data[i]['src'], 3)\n",
        "  TextRank.append(' '.join(test_data[i]['src'][j] for j in TextRank_list))\n",
        "print(len(TextRank))"
      ],
      "metadata": {
        "id": "k5GWDi1QYF8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6212f3a3-5b1f-4419-f316-935c31a86a6f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ROUGE.get_scores(TextRank,target,avg = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQCZ_J_Y3vSr",
        "outputId": "a0d3e6ae-34e6-4d06-8aa4-78b6f60a3fec"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.3941079978866688, 'p': 0.3451499636536774, 'f': 0.35560674411812565}, 'rouge-2': {'r': 0.16133589018200906, 'p': 0.1284649842867144, 'f': 0.13682801333017458}, 'rouge-l': {'r': 0.35721243265278224, 'p': 0.3133145153394227, 'f': 0.3225502757280641}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrhHLMb9G4tC"
      },
      "source": [
        "##Glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwOjBVEjG6nG",
        "outputId": "5170f29a-0b0c-4f97-9527-9e75feb0465b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 100.0% 376.0/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "glv_vector = gensim.downloader.load('glove-wiki-gigaword-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT2f-S5IHiiw",
        "outputId": "897d484b-68ed-473e-eeeb-7097e37d11e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11489\n"
          ]
        }
      ],
      "source": [
        "LSA = []\n",
        "for i in range(len(test_data)):\n",
        "  LSA_list = generate_summary_LSA(test_data[i]['src'], 3, glv_vector)\n",
        "  LSA_list.sort()\n",
        "  LSA.append(' '.join(test_data[i]['src'][j] for j in LSA_list))\n",
        "print(len(LSA))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blp8JBbSJtOe",
        "outputId": "ac3436fc-8525-49bb-92a1-4affe8978142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.24063213984886514, 'p': 0.21960174531222054, 'f': 0.22022012809860664}, 'rouge-2': {'r': 0.05469166572105845, 'p': 0.04576662555625529, 'f': 0.04716799338373901}, 'rouge-l': {'r': 0.21460218641167286, 'p': 0.19626618195615225, 'f': 0.19657528467796434}}\n"
          ]
        }
      ],
      "source": [
        "print(ROUGE.get_scores(LSA,target,avg = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNyJmpUlJ6Af"
      },
      "outputs": [],
      "source": [
        "Cluster = []\n",
        "for i in range(len(test_data)):\n",
        "  Cluster_list = generate_summary_Cluster(test_data[i]['src'], 3,glv_vector)\n",
        "  Cluster_list.sort()\n",
        "  Cluster.append(' '.join(test_data[i]['src'][j] for j in Cluster_list))\n",
        "print(len(Cluster))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY2zGhIpJ8eG"
      },
      "outputs": [],
      "source": [
        "print(ROUGE.get_scores(Cluster,target,avg = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-FMNMO_J-lQ"
      },
      "outputs": [],
      "source": [
        "TextRank = []\n",
        "for i in range(len(test_data)):\n",
        "  TextRank_list = generate_summary_TextRank(test_data[i]['src'], 3,glv_vector)\n",
        "  TextRank_list.sort()\n",
        "  TextRank.append(' '.join(test_data[i]['src'][j] for j in TextRank_list))\n",
        "print(len(TextRank))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vALa7w3uKBID"
      },
      "outputs": [],
      "source": [
        "print(ROUGE.get_scores(TextRank,target,avg = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XOBQOPnKDUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25ciqVyccIn3"
      },
      "source": [
        "##FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA9aBBKAcKV5"
      },
      "outputs": [],
      "source": [
        "ft_vector = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HzTzUbGcTCN"
      },
      "outputs": [],
      "source": [
        "LSA = []\n",
        "for i in range(len(test_data)):\n",
        "  LSA_list = generate_summary_LSA(preprocesed_test_data[i], 3, ft_vector)\n",
        "  LSA_list.sort()\n",
        "  LSA.append(' '.join(test_data[i]['src'][j] for j in LSA_list))\n",
        "print(len(LSA))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzmFhaLrcXUp"
      },
      "outputs": [],
      "source": [
        "print(ROUGE.get_scores(LSA,target,avg = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTXnT_VFcYwi"
      },
      "outputs": [],
      "source": [
        "Cluster = []\n",
        "for i in range(len(test_data)):\n",
        "  Cluster_list = generate_summary_Cluster(preprocesed_test_data[i], 3,ft_vector)\n",
        "  Cluster_list.sort()\n",
        "  Cluster.append(' '.join(test_data[i]['src'][j] for j in Cluster_list))\n",
        "print(len(Cluster))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3qKmwfecao3"
      },
      "outputs": [],
      "source": [
        "print(ROUGE.get_scores(Cluster,target,avg = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEuV3ZSicb6C"
      },
      "outputs": [],
      "source": [
        "TextRank = []\n",
        "for i in range(len(test_data)):\n",
        "  TextRank_list = generate_summary_TextRank(test_data[i]['src'], 3,ft_vector)\n",
        "  TextRank_list.sort()\n",
        "  TextRank.append(' '.join(test_data[i]['src'][j] for j in TextRank_list))\n",
        "print(len(TextRank))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX7Vd0WscdY9"
      },
      "outputs": [],
      "source": [
        "print(ROUGE.get_scores(TextRank,target,avg = True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}